# spark_jobs/process_test_data.py (FINAL UPDATED for Dynamic Model Reloading, Metrics, and Correct Model Input)
from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col, current_timestamp, udf, array, concat_ws, when, lit, concat, create_map
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, ArrayType
from itertools import chain
from functools import reduce
from pyspark.ml import PipelineModel
from sklearn.metrics import f1_score, accuracy_score
import numpy as np
from datetime import datetime
import logging
import os

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s') # Added format for clarity
logger = logging.getLogger(__name__)

# --- Spark Session Configuration ---
SPARK_PACKAGES = "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,com.datastax.spark:spark-cassandra-connector_2.12:3.5.0"

spark = SparkSession.builder \
    .appName("KafkaTestDataProcessorAndPredictor") \
    .config("spark.jars.packages", SPARK_PACKAGES) \
    .config("spark.cassandra.connection.host", "cassandra") \
    .config("spark.local.dir", "/opt/spark_temp_data") \
    .getOrCreate()

spark.sparkContext.setLogLevel("WARN")
logger.info("Spark Session for Test Data Processor created.")

# --- Configuration Variables ---
MODEL_PATH = "/opt/bitnami/spark/jobs/model/sentiment_model" # Shared volume path for model
LABELS_FILE_NAME = "labels.json" # File to store the StringIndexer labels (saved by trainer)
KAFKA_BOOTSTRAP_SERVERS = "kafka:29092" # Make sure this matches your Kafka cluster setup (e.g., kafka-1:9092,kafka-2:9092)
RAW_TEST_TOPIC = "raw_test_data"
PREDICTED_KAFKA_TOPIC = "predicted_test_data"
REALTIME_METRICS_TOPIC = "realtime_metrics"
CASSANDRA_TRAIN_KEYSPACE = "bigdata_keyspace"
CASSANDRA_TRAIN_TABLE = "raw_train_data" # This is the table for raw test data for retraining

aspect_cols = ['BATTERY', 'CAMERA', 'DESIGN', 'FEATURES', 'GENERAL', 'PERFORMANCE', 'PRICE', 'SCREEN', 'SER&ACC', 'STORAGE', 'OTHERS']

test_data_kafka_schema = StructType([
    StructField("id", StringType(), True),# UUID generated by producer
    StructField("comment", StringType(), True),
    StructField("n_star", IntegerType(), True),
    StructField("date_time", StringType(), True), # Comes as string, will be cast to TimestampType
    StructField("label", StringType(), True),
])

def load_latest_model():
    models = {
        "BATTERY": {
            "model": None
        },
        "CAMERA": {
            "model": None
        },
        "DESIGN": {
            "model": None
        },
        "FEATURES": {
            "model": None
        },
        "GENERAL": {
            "model": None
        },
        "PERFORMANCE": {
            "model": None
        },
        "PRICE": {
            "model": None
        },
        "SCREEN": {
            "model": None
        },
        "SER&ACC": {
            "model": None
        },
        "STORAGE": {
            "model": None
        },
        "OTHERS": {
            "model": None
        },
    }
    
    for aspect in aspect_cols:
        ASPECT_MODEL_PATH = os.path.join(MODEL_PATH, aspect)
        model = models[aspect]["model"]
        if not os.path.exists(ASPECT_MODEL_PATH):
            logger.error(f"Model path {ASPECT_MODEL_PATH} does not exist. Cannot load model.")
            model = None
            return

        model = PipelineModel.load(ASPECT_MODEL_PATH)
        models[aspect]['model'] = model
        logger.info(f"{aspect} model is loaded sucessfully.")
    
    return models

def build_aspect_string_expr(aspect_cols):
    sentiment_map_expr = create_map([lit(x) for x in chain(*{
        1: "Negative", 2: "Neutral", 3: "Positive"
    }.items())])

    conditions = []
    for a in aspect_cols:
        p = f"{a}_pred"
        if a.upper() == "OTHERS":
            conditions.append(when(col(p) != 0, lit("{OTHERS};")))
        else:
            sentiment_str_col = sentiment_map_expr.getItem(col(p).cast("int"))
            formatted = concat(lit("{"), concat_ws("#", lit(a), sentiment_str_col), lit("};"))
            conditions.append(when(col(p) != 0, formatted))
    
    return concat_ws("", *conditions)

sentiment_map = {"Negative": 1, "Neutral": 2, "Positive": 3}

# Chuyển label string thành array của int theo thứ tự aspect_cols
def parse_label_string(label_str):
    aspect_sentiments = {a: 0 for a in aspect_cols}
    if not label_str:
        return list(aspect_sentiments.values())
    parts = label_str.strip().split(";")
    for part in parts:
        if part == "{OTHERS}":
            aspect_sentiments["OTHERS"] = 1
        elif part.startswith("{") and "#" in part:
            try:
                content = part.strip("{}")
                aspect, sent = content.split("#")
                if aspect in aspect_sentiments:
                    aspect_sentiments[aspect] = sentiment_map.get(sent, 0)
            except:
                continue
    return [aspect_sentiments[a] for a in aspect_cols]

parse_label_udf = udf(parse_label_string, ArrayType(IntegerType()))

logger.info(f"Reading stream from Kafka topic: {RAW_TEST_TOPIC}")

# Read data from Kafka (raw_test_data topic)
kafka_df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", KAFKA_BOOTSTRAP_SERVERS) \
    .option("subscribe", RAW_TEST_TOPIC) \
    .option("startingOffsets", "latest") \
    .option("maxOffsetsPerTrigger", 1024) \
    .load()

# Parse the JSON 'value' from Kafka, cast date_time to Timestamp, add processing_timestamp
parsed_test_df = kafka_df.selectExpr("CAST(value AS STRING)") \
    .select(from_json(col("value"), test_data_kafka_schema).alias("data")) \
    .select("data.*") \
    .withColumn("id", col("id")) \
    .withColumn("date_time", col("date_time").cast(TimestampType())) \
    .withColumn("processing_timestamp", current_timestamp())

# --- ForeachBatch Function for Processing and Saving ---
def process_batch_with_model(batch_df, batch_id):
    if batch_df.count() == 0:
        logger.debug(f"Batch {batch_id}: No records in batch. Skipping.")
        return

    # Check for and load latest model and labels before processing each batch
    models = load_latest_model()

    for aspect in aspect_cols:
        model = models[aspect]["model"]
        if model is None:
            logger.error(f"Batch {batch_id}: No {aspect} model available for predictions. Skipping batch.")
            return

    logger.info(f"Batch {batch_id}: Processing {batch_df.count()} records with model.")
    
    try:
        pred_dfs = []
        for aspect in aspect_cols:
            model = models[aspect]["model"]
            pred_df = model.transform(batch_df).select("id", col("prediction").alias(f"{aspect}_pred"))
            pred_dfs.append(pred_df)
        
        predictions_df = reduce(
            lambda df1, df2: df1.join(df2, on="id", how="left"),
            pred_dfs,
            batch_df.select("id", "comment", "n_star", "date_time", "label", "processing_timestamp")  # base columns
        )
            
        pred_cols = [f"{a}_pred" for a in aspect_cols]
        predictions_df = predictions_df.withColumn("prediction_array", array(*[col(c).cast("int") for c in pred_cols]))
        predictions_df = predictions_df.withColumn("predicted_label", build_aspect_string_expr(aspect_cols))
        
        # Prepare data for Kafka and Cassandra output
        output_prepared_df = predictions_df.select(
            "id",
            "comment",
            "n_star",
            "date_time",
            "label", # Original true label (from Kafka input)
            "predicted_label", # Predicted string label (from IndexToString in model)
            "processing_timestamp" # Include processing timestamp
        )

        retrain_df = batch_df.withColumnRenamed("processing_timestamp", "ingestion_timestamp") \
            .select(
                "id",
                "comment",
                "n_star",
                "date_time",
                "label",
                "ingestion_timestamp"
            )

        # --- Write Predictions to Kafka ---
        logger.info(f"Batch {batch_id}: Writing predictions to Kafka topic: {PREDICTED_KAFKA_TOPIC}")
        output_prepared_df \
            .selectExpr("CAST(id AS STRING) AS key", "to_json(struct(*)) AS value") \
            .write \
            .format("kafka") \
            .option("kafka.bootstrap.servers", KAFKA_BOOTSTRAP_SERVERS) \
            .option("topic", PREDICTED_KAFKA_TOPIC) \
            .save() 

        # --- Write raw test data to Cassandra (for future retraining) ---
        logger.info(f"Batch {batch_id}: Writing raw test data for retraining to Cassandra: {CASSANDRA_TRAIN_KEYSPACE}.{CASSANDRA_TRAIN_TABLE}")
        retrain_df.write \
            .format("org.apache.spark.sql.cassandra") \
            .mode("append") \
            .options(table=CASSANDRA_TRAIN_TABLE, keyspace=CASSANDRA_TRAIN_KEYSPACE) \
            .save()

    
        # --- Real-time Metrics Calculation and Output ---
        predictions_df = predictions_df.withColumn("true_array", parse_label_udf(col("label")))
        eval_df = predictions_df.select("true_array", "prediction_array").toPandas()
        y_true = np.array(eval_df["true_array"].to_list())
        y_pred = np.array(eval_df["prediction_array"].to_list())

        if len(eval_df) == 0:
            logger.warning(f"Batch {batch_id}: No valid records for metrics calculation. Skipping metrics.")
        else:
            logger.info(f"Batch {batch_id}: Calculating advanced metrics for {eval_df.count()} records.")

            # Tính chỉ số
            f1_micro = f1_score(y_true.flatten(), y_pred.flatten(), average='micro', zero_division=0)
            f1_macro = f1_score(y_true.flatten(), y_pred.flatten(), average='macro', zero_division=0)
            accuracy = accuracy_score(y_true.flatten(), y_pred.flatten())
            exact_match = (y_true == y_pred).all(axis=1).mean()

            logger.info(f"F1 (micro): {f1_micro:.4f}, F1 (macro): {f1_macro:.4f}, Accuracy per label: {accuracy:.4f}, Exact match: {exact_match:.4f}")

            # 4. Gửi lên Kafka nếu muốn
            metrics_data = {
                "batch_id": batch_id,
                "timestamp": datetime.now().isoformat(),
                "accuracy_per_label": float(accuracy),
                "exact_match_accuracy": float(exact_match),
                "f1_micro": float(f1_micro),
                "f1_macro": float(f1_macro),
                "total_records": predictions_df.count()
            }
            metrics_df = spark.createDataFrame([metrics_data])
            metrics_df.selectExpr("to_json(struct(*)) AS value") \
                .write \
                .format("kafka") \
                .option("kafka.bootstrap.servers", KAFKA_BOOTSTRAP_SERVERS) \
                .option("topic", REALTIME_METRICS_TOPIC) \
                .save()

    except Exception as e:
        logger.error(f"Batch {batch_id}: Error processing batch or writing outputs: {e}", exc_info=True)


# --- Apply the foreachBatch sink to the streaming DataFrame ---
query_test_processor = parsed_test_df.writeStream \
    .foreachBatch(process_batch_with_model) \
    .option("checkpointLocation", os.path.join("/tmp/spark/checkpoints", RAW_TEST_TOPIC + "_processor_checkpoint")) \
    .trigger(processingTime="60 seconds") \
    .start()

logger.info("Spark Structured Streaming query for Test Data Processor started.")

# Await termination to keep the Spark application running
query_test_processor.awaitTermination()

spark.stop()
logger.info("Spark Session stopped.")