# spark_jobs/process_train_data.py
from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col, current_timestamp, udf
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType
import uuid
import logging
import os
import time # Import time for delays between retries

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# --- Spark Session Configuration ---
# Packages needed for Kafka and Cassandra
SPARK_PACKAGES = "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,com.datastax.spark:spark-cassandra-connector_2.12:3.5.0"

spark = SparkSession.builder \
    .appName("KafkaToCassandraTrainingIngestion") \
    .config("spark.jars.packages", SPARK_PACKAGES) \
    .config("spark.cassandra.connection.host", "cassandra") \
    .config("spark.local.dir", "/opt/spark_temp_data") \
    .getOrCreate()

spark.sparkContext.setLogLevel("WARN") # Keep Spark's internal logs concise
logger.info("Spark Session for Kafka to Cassandra Training Data Ingestion created.")

# --- Configuration Variables ---
KAFKA_BOOTSTRAP_SERVERS = "kafka:29092"
KAFKA_SOURCE_TOPIC = "raw_train_data" # Input topic for raw training data
CASSANDRA_KEYSPACE = "bigdata_keyspace"
CASSANDRA_TABLE = "raw_train_data" # Your existing training data table in Cassandra

# --- Define Schema for Incoming Raw Data from Kafka ---
raw_data_kafka_schema = StructType([
    StructField("id", StringType(), True),# UUID generated by producer
    StructField("comment", StringType(), True),
    StructField("n_star", IntegerType(), True),
    StructField("date_time", StringType(), True), # Comes as string, will be cast to TimestampType
    StructField("label", StringType(), True),
])

logger.info(f"Reading stream from Kafka topic: {KAFKA_SOURCE_TOPIC}")

# Read data from Kafka (raw_train_data topic)
kafka_df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", KAFKA_BOOTSTRAP_SERVERS) \
    .option("subscribe", KAFKA_SOURCE_TOPIC) \
    .option("startingOffsets", "earliest") \
    .option("maxOffsetsPerTrigger", 1024) \
    .load()

# Parse the JSON 'value' from Kafka for basic structure.
raw_kafka_values_df = kafka_df.selectExpr("CAST(value AS STRING) as json_value")

# --- ForeachBatch Function for Processing and Saving Training Data ---
def process_training_batch(batch_df, batch_id):
    """
    Processes each micro-batch of raw training data, parses it, and writes to Cassandra.
    Includes retry logic for Cassandra write operations.
    """
    if batch_df.count() == 0:
        logger.debug(f"Batch {batch_id}: No records in batch. Skipping processing for {KAFKA_SOURCE_TOPIC}.")
        return

    logger.info(f"Batch {batch_id}: Processing {batch_df.count()} records from Kafka topic '{KAFKA_SOURCE_TOPIC}'.")

    # Parse the JSON 'value' from Kafka, cast date_time to Timestamp, add ingestion_timestamp
    parsed_batch_df = batch_df.select(from_json(col("json_value"), raw_data_kafka_schema).alias("data")) \
        .select("data.*") \
        .withColumn("date_time", col("date_time").cast(TimestampType())) \
        .withColumn("ingestion_timestamp", current_timestamp()) # Timestamp when Spark ingested it

    logger.info(f"Batch {batch_id}: Parsed DataFrame schema for Cassandra ingestion:")
    parsed_batch_df.printSchema()

    logger.info(f"Batch {batch_id}: Writing parsed training data to Cassandra: {CASSANDRA_KEYSPACE}.{CASSANDRA_TABLE}")
                
    # Write to Cassandra using 'append' mode.
    # Cassandra handles deduplication based on the Primary Key ('id' column).
    # If a record with the same ID is sent, it will be upserted (updated if exists, inserted if new).
    parsed_batch_df.write \
        .format("org.apache.spark.sql.cassandra") \
        .mode("append") \
        .options(table=CASSANDRA_TABLE, keyspace=CASSANDRA_KEYSPACE) \
        .save()
    
    logger.info(f"Batch {batch_id}: Successfully ingested {parsed_batch_df.count()} records to Cassandra.")


# --- Apply the foreachBatch sink to the streaming DataFrame ---
query_cassandra_sink = raw_kafka_values_df.writeStream \
    .foreachBatch(process_training_batch) \
    .option("checkpointLocation", "/tmp/spark/checkpoints/" + KAFKA_SOURCE_TOPIC + "_to_cassandra_checkpoint") \
    .trigger(processingTime="20 seconds") \
    .start()

logger.info("Spark Structured Streaming query for Kafka to Cassandra ingestion started.")

# Await termination of the query to keep the script running
query_cassandra_sink.awaitTermination()

spark.stop()
logger.info("Spark Session stopped.")