version: '3.8'

services:
  # --- Zookeeper for Kafka ---
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    hostname: zookeeper
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    networks:
      - spark-net
    healthcheck:
      test: ["CMD", "sh", "-c", "nc -z localhost 2181"]
      interval: 10s
      timeout: 5s
      retries: 5

  # --- Kafka Broker ---
  kafka:
    image: confluentinc/cp-kafka:7.4.0
    hostname: kafka
    container_name: kafka
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_LOG_RETENTION_HOURS: 1
      KAFKA_LOG_RETENTION_BYTES: 1073741820  # Max 1GB per partition
      KAFKA_LOG_SEGMENT_BYTES: 1073741820
      KAFKA_LOG_CLEANUP_POLICY: delete # Ensure it's set to delete old segments
      KAFKA_LOG_CLEANER_ENABLE: "true"
    networks:
      - spark-net
    depends_on:
      zookeeper:
        condition: service_healthy
    volumes:
      - kafka_data:/var/lib/kafka/data

  # --- Cassandra Database ---
  cassandra:
    image: cassandra:4.1
    hostname: cassandra
    container_name: cassandra
    ports:
      - "9042:9042" # CQL native port
    environment:
      CASSANDRA_CLUSTER_NAME: 'MySparkCassandraCluster'
      CASSANDRA_NUM_TOKENS: 256
      CASSANDRA_DC: 'datacenter1'
      CASSANDRA_RACK: 'rack1'
    volumes:
      - cassandra_data:/var/lib/cassandra
    networks:
      - spark-net
    restart: always
    healthcheck: # Healthcheck for Cassandra
      test: ["CMD-SHELL", "cqlsh --debug localhost 9042 -e 'DESCRIBE KEYSPACES;' || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 10 # Cassandra can take a while to become fully available
      start_period: 60s # Give it initial time to start

  # --- Spark Master ---
  spark-master:
    # image: bitnami/spark:3.5.0 # Using Bitnami image which includes pre-installed dependencies
    build:
      context: .
      dockerfile: dockerfiles/spark/Dockerfile
      args:
        SPARK_VERSION: 3.5.0
        SPARK_SCALA_VERSION: 2.12
    hostname: spark-master
    container_name: spark-master
    command: bin/spark-class org.apache.spark.deploy.master.Master -h spark-master
    ports:
      - "8080:8080" # Spark Master Web UI
      - "7077:7077" # Spark Master internal communication
    environment:
      SPARK_MODE: master
      SPARK_RPC_AUTHENTICATION_ENABLED: "no" # For simplicity in dev
      SPARK_RPC_ENCRYPTION_ENABLED: "no"
      SPARK_LOCAL_DIRS: /tmp/spark
      SPARK_LOG_LEVEL: INFO # Or WARN/ERROR for less verbosity
    networks:
      - spark-net
    volumes:
      - ./spark_jobs:/opt/bitnami/spark/jobs # Mount your Spark jobs
      - ./data:/opt/bitnami/spark/data # Mount your data folder for Spark access
    healthcheck:
      test: ["CMD-SHELL", "wget -q -O /dev/null http://localhost:8080"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s

  # --- Spark Worker 1 ---
  spark-worker-1: # Renamed from spark-worker
    build:
      context: .
      dockerfile: dockerfiles/spark/Dockerfile
      args:
        SPARK_VERSION: 3.5.0
        SPARK_SCALA_VERSION: 2.12
    hostname: spark-worker-1 # Unique hostname
    container_name: spark-worker-1 # Unique container name
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    environment:
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_WORKER_CORES: 1
      SPARK_WORKER_MEMORY: 2G
      SPARK_RPC_AUTHENTICATION_ENABLED: "no"
      SPARK_RPC_ENCRYPTION_ENABLED: "no"
      SPARK_LOCAL_DIRS: /tmp/spark
      SPARK_LOG_LEVEL: INFO
    networks:
      - spark-net
    volumes:
      - ./spark_jobs:/opt/bitnami/spark/jobs
      - ./data:/opt/bitnami/spark/data

  # --- Spark Worker 2 ---
  spark-worker-2: # New worker service
    build:
      context: .
      dockerfile: dockerfiles/spark/Dockerfile
      args:
        SPARK_VERSION: 3.5.0
        SPARK_SCALA_VERSION: 2.12
    hostname: spark-worker-2
    container_name: spark-worker-2
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    environment:
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_WORKER_CORES: 1
      SPARK_WORKER_MEMORY: 2G
      SPARK_RPC_AUTHENTICATION_ENABLED: "no"
      SPARK_RPC_ENCRYPTION_ENABLED: "no"
      SPARK_LOCAL_DIRS: /tmp/spark
      SPARK_LOG_LEVEL: INFO
    networks:
      - spark-net
    volumes:
      - ./spark_jobs:/opt/bitnami/spark/jobs
      - ./data:/opt/bitnami/spark/data

  # --- Spark Worker 3 ---
  spark-worker-3: # New worker service
    build:
      context: .
      dockerfile: dockerfiles/spark/Dockerfile
      args:
        SPARK_VERSION: 3.5.0
        SPARK_SCALA_VERSION: 2.12
    hostname: spark-worker-3
    container_name: spark-worker-3
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    environment:
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_WORKER_CORES: 1
      SPARK_WORKER_MEMORY: 2G
      SPARK_RPC_AUTHENTICATION_ENABLED: "no"
      SPARK_RPC_ENCRYPTION_ENABLED: "no"
      SPARK_LOCAL_DIRS: /tmp/spark
      SPARK_LOG_LEVEL: INFO
    networks:
      - spark-net
    volumes:
      - ./spark_jobs:/opt/bitnami/spark/jobs
      - ./data:/opt/bitnami/spark/data

  # --- Spark Worker 4 ---
  spark-worker-4: # New worker service
    build:
      context: .
      dockerfile: dockerfiles/spark/Dockerfile
      args:
        SPARK_VERSION: 3.5.0
        SPARK_SCALA_VERSION: 2.12
    hostname: spark-worker-4
    container_name: spark-worker-4
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    environment:
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_WORKER_CORES: 1
      SPARK_WORKER_MEMORY: 2G
      SPARK_RPC_AUTHENTICATION_ENABLED: "no"
      SPARK_RPC_ENCRYPTION_ENABLED: "no"
      SPARK_LOCAL_DIRS: /tmp/spark
      SPARK_LOG_LEVEL: INFO
    networks:
      - spark-net
    volumes:
      - ./spark_jobs:/opt/bitnami/spark/jobs
      - ./data:/opt/bitnami/spark/data
    
  # --- Spark Worker 5 ---
  spark-worker-5: # New worker service
    build:
      context: .
      dockerfile: dockerfiles/spark/Dockerfile
      args:
        SPARK_VERSION: 3.5.0
        SPARK_SCALA_VERSION: 2.12
    hostname: spark-worker-5
    container_name: spark-worker-5
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    environment:
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_WORKER_CORES: 1
      SPARK_WORKER_MEMORY: 2G
      SPARK_RPC_AUTHENTICATION_ENABLED: "no"
      SPARK_RPC_ENCRYPTION_ENABLED: "no"
      SPARK_LOCAL_DIRS: /tmp/spark
      SPARK_LOG_LEVEL: INFO
    networks:
      - spark-net
    volumes:
      - ./spark_jobs:/opt/bitnami/spark/jobs
      - ./data:/opt/bitnami/spark/data

  # --- Spark Worker 6 ---
  spark-worker-6: # New worker service
    build:
      context: .
      dockerfile: dockerfiles/spark/Dockerfile
      args:
        SPARK_VERSION: 3.5.0
        SPARK_SCALA_VERSION: 2.12
    hostname: spark-worker-6
    container_name: spark-worker-6
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    environment:
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_WORKER_CORES: 1
      SPARK_WORKER_MEMORY: 2G
      SPARK_RPC_AUTHENTICATION_ENABLED: "no"
      SPARK_RPC_ENCRYPTION_ENABLED: "no"
      SPARK_LOCAL_DIRS: /tmp/spark
      SPARK_LOG_LEVEL: INFO
    networks:
      - spark-net
    volumes:
      - ./spark_jobs:/opt/bitnami/spark/jobs
      - ./data:/opt/bitnami/spark/data

  # --- Spark Worker 7 ---
  spark-worker-7: # New worker service
    build:
      context: .
      dockerfile: dockerfiles/spark/Dockerfile
      args:
        SPARK_VERSION: 3.5.0
        SPARK_SCALA_VERSION: 2.12
    hostname: spark-worker-7
    container_name: spark-worker-7
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    environment:
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_WORKER_CORES: 1
      SPARK_WORKER_MEMORY: 2G
      SPARK_RPC_AUTHENTICATION_ENABLED: "no"
      SPARK_RPC_ENCRYPTION_ENABLED: "no"
      SPARK_LOCAL_DIRS: /tmp/spark
      SPARK_LOG_LEVEL: INFO
    networks:
      - spark-net
    volumes:
      - ./spark_jobs:/opt/bitnami/spark/jobs
      - ./data:/opt/bitnami/spark/data

  # --- Spark Worker 8 ---
  spark-worker-8: # New worker service
    build:
      context: .
      dockerfile: dockerfiles/spark/Dockerfile
      args:
        SPARK_VERSION: 3.5.0
        SPARK_SCALA_VERSION: 2.12
    hostname: spark-worker-8
    container_name: spark-worker-8
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    environment:
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_WORKER_CORES: 1
      SPARK_WORKER_MEMORY: 2G
      SPARK_RPC_AUTHENTICATION_ENABLED: "no"
      SPARK_RPC_ENCRYPTION_ENABLED: "no"
      SPARK_LOCAL_DIRS: /tmp/spark
      SPARK_LOG_LEVEL: INFO
    networks:
      - spark-net
    volumes:
      - ./spark_jobs:/opt/bitnami/spark/jobs
      - ./data:/opt/bitnami/spark/data

  spark-train-ingestion:
    image: bitnami/spark:3.5.0
    container_name: spark-train-ingestion
    environment:
      - SPARK_MASTER_URL=spark://spark-master:7077
    volumes:
      - ./spark_jobs:/opt/bitnami/spark/jobs
      - spark_local_data:/opt/spark_temp_data
      - spark_checkpoints_volume:/tmp/spark/checkpoints
    networks:
      - spark-net
    restart: on-failure

  continuous-model-trainer:
    image: bitnami/spark:3.5.0
    container_name: continuous-model-trainer
    environment:
      - SPARK_MASTER_URL=spark://spark-master:7077
    volumes:
      - ./spark_jobs:/opt/bitnami/spark/jobs
      - spark_local_data:/opt/spark_temp_data
      - spark_checkpoints_volume:/tmp/spark/checkpoints
    networks:
      - spark-net
    restart: on-failure

  spark-test-processor:
    image: bitnami/spark:3.5.0
    container_name: spark-test-processor
    environment:
      - SPARK_MASTER_URL=spark://spark-master:7077
    volumes:
      - ./spark_jobs:/opt/bitnami/spark/jobs
      - spark_local_data:/opt/spark_temp_data
      - spark_checkpoints_volume:/tmp/spark/checkpoints
    networks:
      - spark-net
    restart: on-failure

  api:
    build:
      context: .
      dockerfile: ./api/Dockerfile_API
    hostname: data-api
    container_name: data-api
    ports:
      - "5000:5000"
    volumes:
      - ./data:/app/data
    networks:
      - spark-net
    restart: on-failure
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
      interval: 10s
      timeout: 5s
      retries: 5

  # --- Streamlit Dashboard ---
  dashboard:
    build:
      context: . # Build from the dashboard directory
      dockerfile: ./dashboard/Dockerfile_Dashboard # Specify custom Dockerfile name
    hostname: streamlit-dashboard
    container_name: streamlit-dashboard
    ports:
      - "8501:8501" # Default Streamlit port
    depends_on:
      - kafka
    networks:
      - spark-net
    restart: on-failure

  cassandra-setup:
    build:
      context: ./db_setup 
      dockerfile: Dockerfile_CassandraSetup
    container_name: cassandra-setup
    depends_on:
      cassandra:
        condition: service_healthy 
    restart: "on-failure"
    volumes:
      - ./db_setup:/app
    networks:
      - spark-net

networks:
  spark-net:
    driver: bridge

volumes:
  cassandra_data: # For Cassandra persistent data
  kafka_data:
  spark_local_data:
  spark_checkpoints_volume: